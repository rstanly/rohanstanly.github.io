---
title: "Customer Segmentation Report"
author: "Rohan Stanly Keecheril"
format: html
editor: visual
---

# Market Segmentation Report

## 1. Introduction

The KTC company would like to segment their customer base based on their characteristics. The company has data of the customers needed for segmentation we will use the segmented data to better improve customer relations.

## 2. Descriptive Mining

Below is the data we use to segment the KTC customer base.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(rattle)
library(magrittr)

building <- TRUE
scoring  <- ! building

crv$seed <- 42 

library(readxl, quietly=TRUE)

 crs$dataset <- read_excel("C:/Users/rstan/Desktop/MBA/T1/Introduction to Business Analytics/DemoKTC.xlsx", guess_max=1e4)

 crs$dataset
```

### 2.1 Data Exploration

We have information regarding 30 customers of KTC company. We have details of their age, gender, income, marital Status, dependents (number of children), financial Status including whether they have a car loan or a mortgage. We have also made sure that the complete data set has been observed.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Action the user selections from the Data tab. 

# The following variable selections have been noted.

crs$input     <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan", "Mortgage")

crs$numeric   <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan", "Mortgage")

crs$categoric <- NULL

crs$target    <- NULL
crs$risk      <- NULL
crs$ident     <- NULL
crs$ignore    <- NULL
crs$weights   <- NULL

# The 'Hmisc' package provides the 'contents' function.

library(Hmisc, quietly=TRUE)

# Obtain a summary of the dataset.

contents(crs$dataset[, c(crs$input, crs$risk, crs$target)])
summary(crs$dataset[, c(crs$input, crs$risk, crs$target)])

# The 'Hmisc' package provides the 'describe' function.

library(Hmisc, quietly=TRUE)

# Generate a description of the dataset.

describe(crs$dataset[, c(crs$input, crs$risk, crs$target)])
# Rattle timestamp: 2025-07-25 13:51:05.8323 x86_64-w64-mingw32 

# The 'mice' package provides the 'md.pattern' function.

```

#### 2.1.1 Age

After exploring the data we see that the minimum age in the customer data set is 22 , the maximum age is 66 and the average age is 45.97

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Display box plots for the selected variables. 

# Use ggplot2 to generate box plot for Age

# Generate a box plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  ggplot2::ggplot(ggplot2::aes(y=Age)) +
  ggplot2::geom_boxplot(ggplot2::aes(x="All"), notch=TRUE, fill="grey") +
  ggplot2::stat_summary(ggplot2::aes(x="All"), fun.y=mean, geom="point", shape=8) +
  ggplot2::xlab("Rattle 2025-Jul-15 08:42:00 rstan") +
  ggplot2::ggtitle("Distribution of Age") +
  ggplot2::theme(legend.position="none")

gridExtra::grid.arrange(p01)
# Display histogram plots for the selected variables. 

# Use ggplot2 to generate histogram plot for Age

# Generate the plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  dplyr::select(Age) %>%
  ggplot2::ggplot(ggplot2::aes(x=Age)) +
  ggplot2::geom_density(lty=3) +
  ggplot2::xlab("Age\n\nRattle 2025-Jul-17 08:59:12 rstan") +
  ggplot2::ggtitle("Distribution of Age") +
  ggplot2::labs(y="Density")
gridExtra::grid.arrange(p01)

```

We see that there are no outliers from the box plot, from the histogram we can see that most of the people are in the 50s to 60s age and the histogram is left skewed, meaning there are less younger people in the data set.

#### 2.1.2 Income

We can see from the summary that the minimum family income is 8877\$, the maximum income is 59804\$ and the average is 28012\$.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Use ggplot2 to generate box plot for Income

# Generate a box plot.

p02 <- crs %>%
  with(dataset[,]) %>%
  ggplot2::ggplot(ggplot2::aes(y=Income)) +
  ggplot2::geom_boxplot(ggplot2::aes(x="All"), notch=TRUE, fill="grey") +
  ggplot2::stat_summary(ggplot2::aes(x="All"), fun.y=mean, geom="point", shape=8) +
  ggplot2::xlab("Rattle 2025-Jul-15 08:42:01 rstan") +
  ggplot2::ggtitle("Distribution of Income") +
  ggplot2::theme(legend.position="none")

# Display the plots.

gridExtra::grid.arrange(p02)

# Use ggplot2 to generate histogram plot for Income

# Generate the plot.

p02 <- crs %>%
  with(dataset[,]) %>%
  dplyr::select(Income) %>%
  ggplot2::ggplot(ggplot2::aes(x=Income)) +
  ggplot2::geom_density(lty=3) +
  ggplot2::xlab("Income\n\nRattle 2025-Jul-17 08:59:12 rstan") +
  ggplot2::ggtitle("Distribution of Income") +
  ggplot2::labs(y="Density")

# Display the plots.

gridExtra::grid.arrange(p02)
```

We see that there are no outliers and the distribution of income is right skewed from this we can see that very few people have incomes in the 30,000\$ and above range. We can also see that most of customers have incomes around 20,000\$.

#### 2.1.3 Gender

We can see that majority (56.7%) of the customers are female.

#### 2.1.4 Marital Status

The data shows that 80% are married in the customer data

#### 2.1.5 Number of children

The data shows that customers has children in the range 0-3, we see from the average that the majority of customers don't have children

#### 2.1.6 Loan

We can see that majority of the customers do not have loans and about 43% has loans

#### 2.1.7 Mortgage

We can see from the data that majority of the customers do not have a mortgage only about 40% of the customers in the data set has a mortgage.

## 3. Segmentation using Clustering

We conduct cluster analysis on the data of customers from the KTC company to group similar customers together in order to identify further patterns.

#### 3.1 Hierarchical Clustering

We do hierarchical clustering to get an idea of the number of clusters we need to do k means clustering

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Hierarchical Cluster 

# Generate a hierarchical cluster from the numeric data.

crs$dataset[, crs$numeric] %>%
  amap::hclusterpar(method="euclidean", link="ward", nbproc=1) ->
crs$hclust

# Time taken: 0.07 secs
# Dendrogram Plot 

# The 'ggplot2' package provides the 'ggplot' function.

library(ggplot2, quietly=TRUE)

# The 'ggdendro' package provides the 'dendro_data' function.

library(ggdendro, quietly=TRUE)

# Generate the dendrogram plot.

ddata <- dendro_data(crs$hclust, type="rectangle")
g <- ggplot(segment(ddata))
g <- g + geom_segment(aes(x = y, y = x, xend = yend, yend = xend))
g <- g + scale_y_discrete(labels = ddata$label$label)
g <- g + labs(x="Height", y="Observation")
g <- g + ggtitle(expression(atop("Cluster Dendrogram DemoKTC.xlsx", atop(italic("Rattle 2025-Jul-14 17:06:41 rstan")))))
print(g)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(readxl)
crs$dataset <- read_excel("C:/Users/rstan/Desktop/MBA/T1/Introduction to Business Analytics/DemoKTC.xlsx", guess_max=1e4)
mydata<-scale(crs$dataset)
d <- dist(mydata, method = "manhattan") # distance matrix
fit <- hclust(d, method="ward") # Clustering
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
#draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=3, border="red")

```

From the dendrogram we decide to reduce the cluster size to 5 when doing k means clustering.

#### 3.2 K means Clustering

K means clustering is an method commonly used to organize observations into similar clusters. We start by doing k means clustering with 5 clusters.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# KMeans 

# Reset the random number seed to obtain the same results each time.

set.seed(crv$seed)

# The 'reshape' package provides the 'rescaler' function.

library(reshape, quietly=TRUE)

# Generate a kmeans cluster of size 5.

crs$kmeans <- kmeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"), 5)

# Report on the cluster characteristics. 

# Cluster sizes:

paste(crs$kmeans$size, collapse=' ')

# Data means:

colMeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"))

# Cluster centers:

crs$kmeans$centers

# Within cluster sum of squares:

crs$kmeans$withinss

# Time taken: 0.00 secs

# Generate a discriminant coordinates plot.

cluster::clusplot(na.omit(crs$dataset[, intersect(crs$input, crs$numeric)]), crs$kmeans$cluster, color=TRUE, shade=TRUE, main='Discriminant Coordinates DemoKTC.xlsx')


```

We see that when doing clustering with 5 clusters that the overlapping between the clusters is high, to find the optimal number of clusters for doing k means clustering we find the elbow plot.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(factoextra)
library(cluster)
library(readxl, quietly=TRUE)
mydata <- crs$dataset
data <- scale(mydata)  
fviz_nbclust(data, kmeans, method = "wss")
set.seed(123)  # For reproducibility
km <- kmeans(data, centers = 3, nstart = 25)
set.seed(123)  # For reproducibility
km <- kmeans(data, centers = 3, nstart = 25)
fviz_cluster(km, data)
data2<-data# duplicating the data
data2$cluster<-km$cluster# writing the cluster membership in to the data
data2$cluster
```

We can see from the elbow plot that the optimal number of clusters is 3

After reducing the cluster size to 3 we can see that the overlapping between clusters have been reduced.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Rattle timestamp: 2025-07-17 12:27:46.081723 x86_64-w64-mingw32 

# KMeans 

# Reset the random number seed to obtain the same results each time.

set.seed(crv$seed)

# The 'reshape' package provides the 'rescaler' function.

library(reshape, quietly=TRUE)

# Generate a kmeans cluster of size 3.

crs$kmeans <- kmeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"), 3)



#4
# Rattle timestamp: 2025-07-17 12:27:46.252969 x86_64-w64-mingw32 

# Report on the cluster characteristics. 

# Cluster sizes:

paste(crs$kmeans$size, collapse=' ')

# Data means:

colMeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"))

# Cluster centers:

crs$kmeans$centers

# Within cluster sum of squares:

crs$kmeans$withinss

# Time taken: 0.00 secs

# Generate a discriminant coordinates plot.

cluster::clusplot(na.omit(crs$dataset[, intersect(crs$input, crs$numeric)]), crs$kmeans$cluster, color=TRUE, shade=TRUE, main='Discriminant Coordinates DemoKTC.xlsx')


#5
data2<-mydata# duplicating the data
cluster_id<-as.vector(unlist(km$cluster))# writing the cluster membership in to the data
data2<-as.data.frame(cbind(data2,cluster_id))

# Group data2 by cluster_id and compute mean for each group
group_means <- aggregate(. ~ cluster_id, data = data2, FUN = mean)

# Split the original data into a list of data frames by cluster_id
grouped_data <- split(data2, data2$cluster_id)

# If we specifically want 3 data sets, we can extract them like this:
data_cluster1 <- grouped_data[[1]]
data_cluster2 <- grouped_data[[2]]
data_cluster3 <- grouped_data[[3]]

# Optionally view the group means
print(group_means)

```

The customer base of KTC company has been segmented into 3 clusters.

#### 3.1 Results of Segmentation using clustering

Cluster one has an average age of 35 with mostly unmarried individuals who most likely have children , they are individuals who have dependents and high likelihood of having loans

Cluster two is has the highest average age of 40 and are mostly married and likely have children

Cluster three has an average age of 58 and has the highest income among the group of 41632\$ they have a low chance of having loans and mortgages.
